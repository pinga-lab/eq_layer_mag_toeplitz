\section{Methodology}

%======================================================================================
\subsection{Classical equivalent layer for magnetic data}
%======================================================================================

Let $\mathbf{d}^{o}$ be the $N \times 1$ observed data vector, whose $i$th element 
is the total-field anomaly $d^{o}_{i}$ produced by arbitrarily magnetized sources
at the position $(x_{i}, y_{i}, z_{i})$, $i =  1, \dots, N $, 
of a right-handed Cartesian coordinate system with $x$-, $y$- and $z$-axis 
pointing to north, east and down, respectively.
We consider that the total-field anomaly data $d^{o}_{i}$ represent the discrete
values of a harmonic function. Besides, we consider that the main geomagnetic field 
direction at the study area can be defined by the unit vector
\begin{equation}
\hat{\mathbf{F}} = \begin{bmatrix}
F_x \\
F_y \\
F_z
\end{bmatrix} =
\begin{bmatrix}
\cos(I_{0}) \, \cos(D_{0}) \\
\cos(I_{0}) \, \sin(D_{0}) \\
\sin(I_{0})
\end{bmatrix} \: ,
\label{eq:unit_vector_F}
\end{equation}
with constant inclination $I_{0}$ and declination $D_{0}$.
In this case, $d^{o}_{i}$ can be approximated by the predicted total-field anomaly
\begin{equation}
\Delta T_{i} = \sum_{j=1}^{M} \, p_{j} a_{ij} \: ,
\label{eq:integral-sum_mag}
\end{equation}
which describes the magnetic induction exerted, at the observation point $(x_{i}, y_{i}, z_{i})$,
by a discrete layer of $M$ dipoles (equivalent sources) defined on the horizontal plane $z = z_{c}$, 
where $p_{j}$ is the magnetic moment intensity (in A~m~$^{2}$)~of the $j$th dipole, 
that has unit volume and is located at the point $(x_{j}, y_{j}, z_{c})$. In equation
\ref{eq:integral-sum_mag}, $a_{ij}$ is the harmonic function
\begin{equation}
a_{ij}
= c_{m} \, \frac{\mu_{0}}{4\pi} \, \hat{\mathbf{F}}^{\top} \mathbf{H}_{ij} \: \hat{\mathbf{h}} \: ,
\label{eq:aij_mag}
\end{equation}
the unit vector
\begin{equation}
\hat{\mathbf{h}} = \begin{bmatrix}
h_x \\
h_y \\
h_z
\end{bmatrix} =
\begin{bmatrix}
\cos(I) \, \cos(D) \\
\cos(I) \, \sin(D) \\
\sin(I)
\end{bmatrix} \: ,
\label{eq:h_hat}
\end{equation}
defines the magnetization direction of all dipoles, with constant inclination $I$ and declination $D$,
$\mu_{0} = 4\pi \, 10^{-7}$ H/m is the magnetic constant, $c_{m} = 10^{9}$ is a factor that transforms
the magnetic induction from Tesla (T) to nanotesla (nT) and $\mathbf{H}_{ij}$ is a $3 \times 3$ matrix 
\begin{equation}
\mathbf{H}_{ij} = \begin{bmatrix}
\partial_{xx} \frac{1}{r_{ij}} & 
\partial_{xy} \frac{1}{r_{ij}} & 
\partial_{xz} \frac{1}{r_{ij}} \\
\partial_{xy} \frac{1}{r_{ij}} & 
\partial_{yy} \frac{1}{r_{ij}} & 
\partial_{yz} \frac{1}{r_{ij}} \\
\partial_{xz} \frac{1}{r_{ij}} & 
\partial_{yz} \frac{1}{r_{ij}} & 
\partial_{zz} \frac{1}{r_{ij}}
\end{bmatrix} \: ,
\label{eq:Hij}
\end{equation}
where $\partial_{\alpha\beta} \frac{1}{r_{ij}}$, $\alpha = x, y, z$, $\beta = x, y, z$, 
represent the second derivatives of the inverse distance function
\begin{equation}
\frac{1}{r_{ij}} = 
\frac{1}{\sqrt{\left(x_{i} - x_{j} \right)^{2} + 
\left(y_{i} - y_{j} \right)^{2} + \left(z_{i} - z_{c} \right)^{2}}}
\label{eq:1_rij}
\end{equation}
with respect to the coordinates of the observation point $(x_{i}, y_{i}, z_{i})$.

Equation \ref{eq:integral-sum_mag} can be rewritten in matrix notation as follows:
\begin{equation}
\mathbf{d}(\mathbf{p}) = \mathbf{A} \mathbf{p} \: ,
\label{eq:predicted-data-vector_mag}
\end{equation}
where $\mathbf{d}(\mathbf{p})$ is the $N \times 1$ predicted data vector with $i$th element defined
be the predicted total-field anomaly $\Delta T_{i}$ (equation \ref{eq:integral-sum_mag}),
$\mathbf{p}$ is the $M \times 1$ parameter vector whose $j$th element is the magnetic moment intensity
$p_{j}$ of the $j$th dipole and $\mathbf{A}$ is the $N \times M$ sensitivity matrix with element 
$ij$ defined by the harmonic function $a_{ij}$ (equation \ref{eq:aij_mag}).
In the classical equivalent-layer technique, the common approach for 
estimating the parameter vector $\mathbf{p}$ from the observed 
total-field anomaly data $\mathbf{d}^{o}$ is solving the least-squares normal equations
\begin{equation}
\mathbf{A}^{\top}\mathbf{A} \: \mathbf{p} = 
\mathbf{A}^{\top} \mathbf{d}^{o} \: .
\label{eq:normal-equations}
\end{equation}
This equation is usually solved by first computing the Cholesky factor of matrix
$\mathbf{A}^{\top}\mathbf{A}$ and then using it to solve the linear system
\citep[][ p. 262]{golub-vanloan2013}. 
The estimated parameter vector obtained by following this approach will be 
referenced throughout this work as the \textit{classical solution}.

%======================================================================================
\subsection{Standard Conjugate Gradient Least Squares (CGLS) method}
%======================================================================================

The computational cost associated with the classical method for estimating the parameter 
vector $\mathbf{p}$ by solving the linear system \ref{eq:normal-equations} can be very high 
or ever prohibitive when dealing with large data sets. In these cases, a well-known alternative
is solving the normal equations (equation \ref{eq:normal-equations}) iteratively by 
using the standard Conjugate Gradient Least Squares (CGLS) method. Below we present a pseudocode 
for the standard CGLS method:

\begin{algorithm}[H]
	Input: $\mathbf{A}$ and $\mathbf{d}^{o}$.
	
	Output: Estimated parameter vector $\tilde{\mathbf{p}}$.
	
	Set $it = 0$, $\tilde{\mathbf{p}}_{(it)} = \mathbf{0}$, $\mathbf{c}_{(it-1)} = \mathbf{0}$, $\beta_{(it)} = 0$, $\mathbf{s}_{(it)} = \mathbf{d}^{o}$ and $\mathbf{r}_{(it)} = \mathbf{A}^{\top} \mathbf{s}_{(it)}$.
	
	1 - If $it > 0$, $\beta_{(it)} = \dfrac{\| \mathbf{r}_{(it)} \|_{2}^{2}}{\| \mathbf{r}_{(it - 1)} \|_{2}^{2}}$
	
	2 - $\mathbf{c}_{(it)} = \mathbf{r}_{(it)} - \beta_{(it)} \, \mathbf{c}_{(it - 1)}$
	
	3 - $\alpha_{(it)} = \dfrac{{\| \mathbf{r}_{(it)}\|_{2}^{2}}}{\| \mathbf{A} \, \mathbf{c}_{(it)} \|_{2}^{2}}$
	
	4 - $\tilde{\mathbf{p}}_{(it + 1)} = \tilde{\mathbf{p}}_{(it)} + \alpha_{(it)} \, \mathbf{c}_{(it)}$
	
	5 - $\mathbf{s}_{(it + 1)} = \mathbf{s}_{(it)} - \alpha_{(it)} \, \mathbf{A} \, \mathbf{c}_{(it)}$
	
	6 - $\mathbf{r}_{(it + 1)} = \mathbf{A}^{\top} \, \mathbf{s}_{(it + 1)}$
	
	7 - $it = it + 1$
	
	8 - Repeat previous steps until convergence.
	
	\caption{Standard CGLS pseudocode \citep[][ p. 166]{aster2019parameter}.}
\label{al:std-cgls-algorithm}
\end{algorithm}

Setting a convergence criteria based on the minimum tolerance of the residuals is a good 
option to carry out this algorithm efficiently and still obtaining very good results. 
Another possibility is to set an invariance to the Euclidean norm of residuals between 
iterations, which would increase algorithm runtime, but with smaller residuals. 
We chose the first option, as we achieve satisfactory results. 
The estimated parameter vector obtained by using the standard CGLS method 
(Algorithm \ref{al:std-cgls-algorithm}) will be referenced throughout this work as 
the \textit{standard CGLS solution}.

Note that the standard CGLS solution (Algorithm \ref{al:std-cgls-algorithm}) requires 
neither inverse matrix nor matrix-matrix product. Instead, it only requires: one matrix-vector 
product out of the loop and two matrix-vector products per iteration (in steps 3 and 6). 
In the following subsections, we will show how to compute these three matrix-vector products 
efficiently by exploring the structure of the sensitivity matrix $\mathbf{A}$.

PAREI AQUI

%======================================================================================
\subsection{Non-symmetric Block-Toeplitz Toeplitz-Block structure of matrix $\mathbf{A}$}
%======================================================================================

Let us consider that the observed total-field anomaly is located on an $N_x \times N_y$ regular grid of points spaced by $\Delta_x$ and $\Delta_y$ along the $x$- and $y$-directions, respectively.
The notation used in this work will be the same as the one presented in \cite{takahashi2020convolutional}, where the authors described the structure of the symmetric Block-Toeplitz Toeplitz-Block matrix of the gravimetric equivalent layer. Here, we also establish a relation between the pair of \emph{matrix coordinates} $(x_i, y_i)$, $i = 1, ..., N$ or $(x_j, y_j)$, $j = 1, ..., M = N $ and a pair of \emph{grid coordinates} $(x_k, y_l)$ given as:
\begin{equation}
x_{i} \equiv x_{k} = x_{1} + \left[ k(i) - 1 \right] \, \Delta x \: , 
\label{eq:xi}
\end{equation}
and
\begin{equation}
y_{i} \equiv y_{l} = y_{1} + \left[ l(i) - 1 \right] \, \Delta y \: .
\label{eq:yi}
\end{equation}
In a $x$-\textit{oriented grid} the indices $i$ (or $j$) relate as integer functions of $k$ and $l$ by:
\begin{equation}
i(k, l) = (l - 1) \, N_{x} + k \: ,
\label{eq:i-x-oriented}
\end{equation}
\begin{equation}
l(i) = \Bigg\lceil \frac{i}{N_{x}} \Bigg\rceil
\label{eq:l-x-oriented}
\end{equation}
and
\begin{equation}
k(i)  = i - \Bigg\lceil \frac{i}{N_{x}} \Bigg\rceil N_{x} + N_{x} \: .
\label{eq:k-x-oriented}
\end{equation}
For  $y$-\textit{oriented grid} they are given by:
\begin{equation}
i(k, l) = (k - 1) \, N_{y} + l \: ,
\label{eq:i-y-oriented}
\end{equation}
\begin{equation}
k(i) = \Bigg\lceil \frac{i}{N_{y}} \Bigg\rceil
\label{eq:k-y-oriented}
\end{equation}
and
\begin{equation}
l(i) = i - \Bigg\lceil \frac{i}{N_{y}} \Bigg\rceil N_{y} + N_{y} \: ,
\label{eq:l-y-oriented}
\end{equation}
where in equations \ref{eq:l-x-oriented} to \ref{eq:l-y-oriented}, $\lceil .\rceil$ is the celing function.
Figure \ref{fig:methodology} shows an example of a grid $N_{x} \times N_{y}$, where $N_{x} = 4$ and $N_{y} = 3$ demonstrating the relation between the \emph{matrix coordinates} with $k(i)$ and $l(i)$ depending on the orientation of the grid.

The $N \times M$ sensitivity matrix  $\mathbf{A}$ (equation \ref{eq:aij_mag}) can be represented as a grid of $Q \times Q$ blocks $\mathbf{A}^q$, $q = -Q+1,...,0,..., Q-1$. Each block  $\mathbf{A}^q$ has $P \times P$ elements $a^{q}_p$ where $p = -P+1^,...,0,...,P-1$

In a $x$-\textit{oriented grid} $q$ and $p$ give the number of blocks ($Q = N_{y}$) and the number of elements of each block ($P = N_{x}$). They can be defined by the integer functions:
\begin{equation}
q(i, j) = \; l(i) - l(j)
\label{eq:q-x-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; k(i) - k(j) \: ,
\label{eq:p-x-oriented}
\end{equation}
where equations \ref{eq:l-x-oriented} and \ref{eq:k-x-oriented} describe $l(i)$ and $l(j)$ and $k(i)$ and $k(j)$, respectively. When using $y$-oriented grids, $q$ and $p$ still define the number of block and block elements, but $Q = N_{x}$ and $P = N_{y}$. Moreover, the integer functions changes to:
\begin{equation}
q(i, j) = \; k(i) - k(j) 
\label{eq:q-y-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; l(i) - l(j) \: ,
\label{eq:p-y-oriented}
\end{equation}
where equation \ref{eq:k-y-oriented} now defines $k(i)$ and $k(j)$ and equation \ref{eq:l-y-oriented} defines $l(i)$ and $l(j)$. Note that equations \ref{eq:q-x-oriented}, \ref{eq:p-x-oriented}, \ref{eq:q-y-oriented} and \ref{eq:q-y-oriented} differs from the ones presented in \cite{takahashi2020convolutional} by the absence of the module.

In both $x$- or $y$-\textit{orientation}, matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) can be rewritten by the indices $q = -Q + 1,...,0,..., Q-1$ defining the number of its blocks:
\begin{equation}
\mathbf{A} = \begin{bmatrix}
\mathbf{A}^{0}   & \mathbf{A}^{-1} & \cdots          & \mathbf{A}^{-Q+1} \\
\mathbf{A}^{1}   & \mathbf{A}^{0}  & \mathbf{A}^{-1} & \vdots           \\ 
\vdots           & \mathbf{A}^{1}  & \ddots          & \mathbf{A}^{-1}   \\
\mathbf{A}^{Q-1} & \cdots          & \mathbf{A}^{1}  & \mathbf{A}^{0}                 
\end{bmatrix}_{N \times N} \: ,
\label{eq:BTTB_A}
\end{equation}
and by indice $p$, where each block has $P \times P$ elements $a^{q}_{p}$, $p = -P + 1,..., 0, \dots, P - 1$:
\begin{equation}
\mathbf{A}^{q} = \begin{bmatrix}
a^{q}_{0}   & a^{q}_{-1} & \cdots     & a^{q}_{-P+1} \\
a^{q}_{1}   & a^{q}_{0}  & a^{q}_{-1} & \vdots           \\ 
\vdots      & a^{q}_{1}  & \ddots     & a^{q}_{-1}   \\
a^{q}_{P-1} & \cdots     & a^{q}_{1}  & a^{q}_{0}                 
\end{bmatrix}_{P \times P} \: ,
\label{eq:Aq_block}
\end{equation}
In general, matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) is a non-symmetric BTTB, i.e., its blocks are non-symmetric ($\mathbf{A}^{-Q+1} \neq \mathbf{A}^{Q-1} $) and its elements also are non-symmetric ($a^{q}_{-1} \neq a^{q}_{1}$). Depending on specific values of the main field direction and the equivalent sources magnetization directions, matrix $\mathbf{A}$ can assume other structures, for example, when $\hat{\mathbf{F}} = [0, 0, 1]$ and $\hat{\mathbf{h}} = [0, 0, 1]$ it becomes symmetric. In this work, we are considering the more commom situation for the matrix $\mathbf{A}$.

Also differently for the symmetric sensitivity matrix described by \cite{takahashi2020convolutional}, the non-symmetric BTTB matrix cannot be reconstructed only by its first column. The construction of the matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) needs four columns: the first and last columns of the first column of blocks and the first and last columns of the last column of blocks. This has a physical implication in the equivalent layer which is not possible to use only one equivalent source to reprduce all elements of matrix $\mathbf{A}$, such as in the gravity case as demonstrared by \cite{takahashi2020convolutional}. Rather, in the magnetic case it takes four equivalent sources positioned at each corner of the equivalent layer. Figure \ref{fig:4_equivalent_sources} shows the positioning of the equivalent sources in a regular grid $N_x = 4$ $N_y = 3$ necessary to calculate the four columns capable of recover the matrix $\mathbf{A}$.

In this work, we propose a different approach, by calculating the first column of all six different components of second derivatives matrices from $\mathbf{H}_{ij}$ (equation \ref{eq:Hi}). These matrices are, in fact, symmetrics or skew-symmetrics BTTBs, meaning that the first column has all elements of each matrix.

By substituting equations \ref{eq:unit_vector_F}, \ref{eq:h_hat} and \ref{eq:Hi} into equation \ref{eq:aij_mag}, it is possible to describe each element of the sensitivity matrix by the second derivative components of $\mathbf{H}_{ij}$:
\begin{equation}
\begin{split}
a_{ij} = c_{m} \, \frac{\mu_{0}}{4\pi} \, (F_x H_{xx} + F_y H_{xy} + F_z H_{xz}) \, h_x  \, + \\
(F_x H_{xy} + F_y H_{yy} + F_z H_{yz}) \, h_y  \, + \\
(F_x H_{xz} + F_y H_{yz} + F_z H_{zz}) \, h_z \: .
\end{split}
\label{eq:aij_mag_expand}
\end{equation}

If we consider that $c_{m} \, \frac{\mu_{0}}{4\pi}$, $\hat{\mathbf{F}}$ (equation \ref{eq:unit_vector_F}) and $\hat{\mathbf{h}}$ (equation\ref{eq:h_hat}) are constants multiplying the second derivatives $\mathbf{H}$ (equation \ref{eq:Hi}), the sensitivity matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) is purely the sum of the components $H_{xx} + H_{xy} + H_{xz} + H_{xy} + H_{yy} + H_{yz} + H_{xz} + H_{yz} + H_{zz}$ multiplied by the respectives constants of each component. Thus, despite $\mathbf{A}$ not being a symmetric BTTB matrix, it can be in fact, written by calculating only the first column of these components.
In the next few sections we will describe each component $\mathbf{H}$ as its own matrix.

%======================================================================================
\subsection{Structure of matrices components $\mathbf{H_{xx}}$, $\mathbf{H_{yy}}$ and $\mathbf{H_{zz}}$}
%======================================================================================

We can describe the elements of $\mathbf{H_{xx}}$, $\mathbf{H_{yy}}$ and $\mathbf{H_{zz}}$ by substituting equations \ref{eq:xi} and \ref{eq:yi} in equation \ref{eq:Hi}  as:
\begin{equation}
h^{xx}_{ij} = \frac{-1}{ \left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{3}{2}}} + 
		\frac{3 (\Delta k_{ij} \, \Delta x )^{2}}{\left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: ,
\label{eq:hxx_mag}
\end{equation}

\begin{equation}
	h^{yy}_{ij} = \frac{-1}{ \left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{3}{2}}} + 
	\frac{3 (\Delta l_{ij} \, \Delta y )^{2}}{\left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: 
	\label{eq:hyy_mag}
\end{equation}
and
\begin{equation}
	h^{zz}_{ij} = \frac{-1}{ \left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{3}{2}}} + 
	\frac{3 (\Delta z )^{2}}{\left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: ,
	\label{eq:hzz_mag}
\end{equation}
where $\Delta z = z_j - z_i$, $\Delta k_{ij} =k(i) - k(j)$ (equations \ref{eq:k-x-oriented} or \ref{eq:k-y-oriented}) and $\Delta l_{ij} = l(i) - l(j)$ (equations \ref{eq:l-x-oriented} or \ref{eq:l-y-oriented}).
The principal components $\mathbf{H_{xx}}$, $\mathbf{H_{yy}}$ and $\mathbf{H_{zz}}$ (equations \ref{eq:hxx_mag}, \ref{eq:hyy_mag} and \ref{eq:hzz_mag}, respectively) of matrix $\mathbf{H}$ (equation \ref{eq:Hi}) are symmetric-Block-Toeplitz symmetric-Toeplitz-Block matrices. This means that $\mathbf{H_{xx}}$, $\mathbf{H_{yy}}$ and $\mathbf{H_{zz}}$ are Toeplitz and symmetric by its blocks and each of the blocks are symmetric Toeplitz matrices. 
For example, $\mathbf{H_{xx}}$ can be described by the \textit{block indice} $q$ that represent the block diagonals of this matrix as a grid of $Q \times Q$ blocks $\mathbf{H}^{q}_\mathbf{xx}$, $q = 0, \dots, Q - 1$:
\begin{equation}
\mathbf{H_{xx}} = \begin{bmatrix}
\mathbf{H}^{0}_\mathbf{xx}  & \mathbf{H}^{1}_\mathbf{xx} & \cdots         & \mathbf{H}^{Q-1}_\mathbf{xx} \\
\mathbf{H}^{1}_\mathbf{xx}  & \mathbf{H}^{0}_\mathbf{xx} & \ddots         & \vdots           \\ 
\vdots           & \ddots         & \ddots         & \mathbf{H}^{1}_\mathbf{xx}   \\
\mathbf{H}^{Q-1}_\mathbf{xx} & \cdots         & \mathbf{H}^{1}_\mathbf{xx} & \mathbf{H}^{0}_\mathbf{xx}                
\end{bmatrix}_{N \times N} \: .
\label{eq:BTTB_Hxx}
\end{equation}
And each diagonal of the blocks are represented by $P \times P$ elements $h^{q}_{p}$, $p = 0, \dots, P - 1$:
\begin{equation}
\mathbf{H}^{q}_\mathbf{xx} = \{h^{q}_p\} = \begin{bmatrix}
h^{q}_{0}   & h^{q}_{1} & \cdots    & h^{q}_{P-1} \\
h^{q}_{1}   & h^{q}_{0} & \ddots    & \vdots           \\ 
\vdots      & \ddots    & \ddots    & h^{q}_{1}   \\
h^{q}_{P-1} & \cdots    & h^{q}_{1} & h^{q}_{0}                 
\end{bmatrix}_{P \times P} \: .
\label{eq:Hxx_block}
\end{equation}
In a $x$-\textit{oriented grid} $Q = N_{x}$, $P = N_{y}$ and $q$ and $p$ can be defined by the functions:
\begin{equation}
q(i, j) = \; \mid l(i) - l(j) \mid
\label{eq:Hxx-q-x-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; \mid k(i) - k(j) \mid \quad ,
\label{eq:Hxx-p-x-oriented}
\end{equation}
where $l(i)$ and $l(j)$ are defined by equation \ref{eq:l-x-oriented} 
and $k(i)$ and $k(j)$ are defined by equation \ref{eq:k-x-oriented}.
For $y$-oriented grids, $Q = N_{x}$, $P = N_{y}$ and the block indices
$q$ and $p$ are defined, respectively, by the following integer functions 
of the matrix indices $i$ and $j$:
\begin{equation}
q(i, j) = \; \mid k(i) - k(j) \mid 
\label{eq:Hxx-q-y-oriented}
\end{equation}
and
\begin{equation}
p(i, j) = \; \mid l(i) - l(j) \mid \quad ,
\label{eq:Hxx-p-y-oriented}
\end{equation}
This struture can also describe matrices $\mathbf{H_{yy}}$ and $\mathbf{H_{zz}}$ in the same manner and they are identical to the structure of the gravity sensitivity matrix from \cite{takahashi2020convolutional}.

%======================================================================================
\subsection{Structure of the components matrices $\mathbf{H_{xy}}$}
%======================================================================================

By substituting equations \ref{eq:xi} and \ref{eq:yi} in equation \ref{eq:Hi}, we can also describe the elements of $\mathbf{H_{xy}}$, as:
\begin{equation}
	h^{xy}_{ij} = \frac{3 (\Delta k_{ij} \, \Delta x )(\Delta l_{ij} \, \Delta y )}{\left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: ,
	\label{eq:hxy_mag}
\end{equation}

The component $\mathbf{H_{xy}}$ (equation \ref{eq:hxy_mag}) of matrix $\mathbf{H}$ (equation \ref{eq:Hi}) are skew symmetric-Block-Toeplitz skew symmetric-Toeplitz-Block matrices. This means that $\mathbf{H_{xy}}$ is Toeplitz and skew symmetric by its blocks and each of the blocks are skew symmetric Toeplitz matrices. 
This way, matrix $\mathbf{H_{xy}}$ can be described by the \textit{block indice} $q$ that represent the block diagonals of this matrix as a grid of $Q \times Q$ blocks $\mathbf{H}^{q}_\mathbf{xy}$, $q = -Q + 1, \dots, 0, \dots, Q - 1$:
\begin{equation}
	\mathbf{H_{xy}} = \begin{bmatrix}
		\mathbf{H}^{0}_\mathbf{xy}  & \mathbf{H}^{-1}_\mathbf{xy} & \cdots         & \mathbf{H}^{-Q+1}_\mathbf{xy} \\
		\mathbf{H}^{1}_\mathbf{xy}  & \mathbf{H}^{0}_\mathbf{xy} & \ddots         & \vdots           \\ 
		\vdots           & \ddots         & \ddots         & \mathbf{H}^{-1}_\mathbf{xy}   \\
		\mathbf{H}^{Q-1}_\mathbf{xy} & \cdots         & \mathbf{H}^{1}_\mathbf{xy} & \mathbf{H}^{0}_\mathbf{xy}                
	\end{bmatrix}_{N \times N} \: .
	\label{eq:BTTB_Hxy}
\end{equation}
And each diagonal of the blocks are represented by $P \times P$ elements $h^{q}_{p}$, $p = -P + 1, \dots, 0, \dots, P - 1$:
\begin{equation}
	\mathbf{H}^{q}_\mathbf{xy} =  \{h^{q}_p\} = \begin{bmatrix}
		h^{q}_{0}   & h^{q}_{-1} & \cdots    & h^{q}_{-P+1} \\
		h^{q}_{1}   & h^{q}_{0} & \ddots    & \vdots           \\ 
		\vdots      & \ddots    & \ddots    & h^{q}_{-1}   \\
		h^{q}_{P-1} & \cdots    & h^{q}_{1} & h^{q}_{0}                 
	\end{bmatrix}_{P \times P} \: .
	\label{eq:Hxy_block}
\end{equation}
In a $x$-\textit{oriented grid} $Q = N_{x}$, $P = N_{y}$ and $q$ and $p$ can be defined by the functions:
\begin{equation}
	q(i, j) = \; l(i) - l(j) 
	\label{eq:Hxy-q-x-oriented}
\end{equation}
and
\begin{equation}
	p(i, j) = \; k(i) - k(j) \quad ,
	\label{eq:Hxy-p-x-oriented}
\end{equation}
where $l(i)$ and $l(j)$ are defined by equation \ref{eq:l-x-oriented} 
and $k(i)$ and $k(j)$ are defined by equation \ref{eq:k-x-oriented}.
For $y$-oriented grids, $Q = N_{x}$, $P = N_{y}$ and the block indices
$q$ and $p$ are defined, respectively, by the following integer functions 
of the matrix indices $i$ and $j$:
\begin{equation}
	q(i, j) = \;  k(i) - k(j)  
	\label{eq:Hxy-q-y-oriented}
\end{equation}
and
\begin{equation}
	p(i, j) = \;  l(i) - l(j)  \quad ,
	\label{eq:Hxy-p-y-oriented}
\end{equation}
Important to clarify that in this case, as a skew symmetric matrix, the values of oposing diagonals have oposing signals, e.g., $\mathbf{H}^{-1}_\mathbf{xy} = -\mathbf{H}^{1}_\mathbf{xy}$ and $h^{q}_{-1} = -h^{q}_{1} $.

%======================================================================================
\subsection{Structure of the components matrices $\mathbf{H_{xz}}$}
%======================================================================================

By substituting equations \ref{eq:xi} and \ref{eq:yi} in equation \ref{eq:Hi}, the elements of $\mathbf{H_{xz}}$, are given by:
\begin{equation}
	h^{xz}_{ij} = \frac{3 (\Delta k_{ij} \, \Delta x)(\Delta z)}{\left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: ,
	\label{eq:hxz_mag}
\end{equation}

The component $\mathbf{H_{xz}}$ (equation \ref{eq:hxz_mag}) of matrix $\mathbf{H}$ (equation \ref{eq:Hi}) are skew symmetric-Block-Toeplitz symmetric-Toeplitz-Block matrices. This means that $\mathbf{H_{xz}}$ is Toeplitz and skew symmetric by its blocks and each of the blocks are symmetric Toeplitz matrices. 
Thus, matrix $\mathbf{H_{xz}}$ can be described by the \textit{block indice} $q$ that represent the block diagonals of this matrix as a grid of $Q \times Q$ blocks $\mathbf{H}^{q}_\mathbf{xz}$, $q = -Q + 1, \dots, 0, \dots, Q - 1$:
\begin{equation}
	\mathbf{H_{xz}} = \begin{bmatrix}
		\mathbf{H}^{0}_\mathbf{xz}  & \mathbf{H}^{-1}_\mathbf{xz} & \cdots         & \mathbf{H}^{-Q+1}_\mathbf{xz} \\
		\mathbf{H}^{1}_\mathbf{xz}  & \mathbf{H}^{0}_\mathbf{xz} & \ddots         & \vdots           \\ 
		\vdots           & \ddots         & \ddots         & \mathbf{H}^{-1}_\mathbf{xz}   \\
		\mathbf{H}^{Q-1}_\mathbf{xz} & \cdots         & \mathbf{H}^{1}_\mathbf{xz} & \mathbf{H}^{0}_\mathbf{xz}                
	\end{bmatrix}_{N \times N} \: .
	\label{eq:BTTB_Hxz}
\end{equation}
And each diagonal of the blocks are represented by $P \times P$ elements $h^{q}_{p}$, $p = 0, \dots, P - 1$:
\begin{equation}
	\mathbf{H}^{q}_\mathbf{xz} =  \{h^{q}_p\} = \begin{bmatrix}
		h^{q}_{0}   & h^{q}_{1} & \cdots    & h^{q}_{P-1} \\
		h^{q}_{1}   & h^{q}_{0} & \ddots    & \vdots           \\ 
		\vdots      & \ddots    & \ddots    & h^{q}_{1}   \\
		h^{q}_{P-1} & \cdots    & h^{q}_{1} & h^{q}_{0}                 
	\end{bmatrix}_{P \times P} \: .
	\label{eq:Hxz_block}
\end{equation}
In a $x$-\textit{oriented grid} $Q = N_{x}$, $P = N_{y}$ and $q$ and $p$ can be defined by the functions:
\begin{equation}
	q(i, j) = \; l(i) - l(j) 
	\label{eq:Hxz-q-x-oriented}
\end{equation}
and
\begin{equation}
	p(i, j) = \; \mid k(i) - k(j) \mid \quad ,
	\label{eq:Hxz-p-x-oriented}
\end{equation}
where $l(i)$ and $l(j)$ are defined by equation \ref{eq:l-x-oriented} 
and $k(i)$ and $k(j)$ are defined by equation \ref{eq:k-x-oriented}.
For $y$-oriented grids, $Q = N_{x}$, $P = N_{y}$ and the block indices
$q$ and $p$ are defined, respectively, by the following integer functions 
of the matrix indices $i$ and $j$:
\begin{equation}
	q(i, j) = \;  k(i) - k(j)  
	\label{eq:Hxz-q-y-oriented}
\end{equation}
and
\begin{equation}
	p(i, j) = \; \mid l(i) - l(j) \mid \quad ,
	\label{eq:Hxz-p-y-oriented}
\end{equation}
In this case as a skew symmetric matrix by blocks, the values of oposing diagonals blocks have oposing signals, e.g., $\mathbf{H}^{-1}_\mathbf{xz} = -\mathbf{H}^{1}_\mathbf{xz}$ but each block is a symmetric matrix.

%======================================================================================
\subsection{Structure of the components matrices $\mathbf{H_{yz}}$}
%======================================================================================

Finally, by substituting equations \ref{eq:xi} and \ref{eq:yi} in equation \ref{eq:Hi}, we can also describe the elements of $\mathbf{H_{yz}}$, as:
\begin{equation}
	h^{yz}_{ij} = \frac{3 (\Delta l_{ij} \, \Delta y )(\Delta z)}{\left[ 
		\left( \Delta k_{ij} \, \Delta x \right)^{2} + 
		\left( \Delta l_{ij} \, \Delta y \right)^{2} + 
		\left( \Delta z \right)^{2} \right]^{\frac{5}{2}}} \: ,
	\label{eq:hyz_mag}
\end{equation}

The component $\mathbf{H_{yz}}$ (equation \ref{eq:hyz_mag}) of matrix $\mathbf{H}$ (equation \ref{eq:Hi}) are symmetric-Block-Toeplitz skew symmetric-Toeplitz-Block matrices. This means that $\mathbf{H_{yz}}$ is Toeplitz and symmetric by its blocks and each of the blocks are skew symmetric Toeplitz matrices.
Thus, matrix $\mathbf{H_{yz}}$ can be described by the \textit{block indice} $q$ that represent the block diagonals of this matrix as a grid of $Q \times Q$ blocks $\mathbf{H}^{q}_\mathbf{yz}$, $q = 0, \dots, Q - 1$:
\begin{equation}
	\mathbf{H_{yz}} = \begin{bmatrix}
		\mathbf{H}^{0}_\mathbf{yz}  & \mathbf{H}^{1}_\mathbf{yz} & \cdots         & \mathbf{H}^{Q-1}_\mathbf{yz} \\
		\mathbf{H}^{1}_\mathbf{yz}  & \mathbf{H}^{0}_\mathbf{yz} & \ddots         & \vdots           \\ 
		\vdots           & \ddots         & \ddots         & \mathbf{H}^{1}_\mathbf{yz}   \\
		\mathbf{H}^{Q-1}_\mathbf{yz} & \cdots         & \mathbf{H}^{1}_\mathbf{yz} & \mathbf{H}^{0}_\mathbf{yz}                
	\end{bmatrix}_{N \times N} \: .
	\label{eq:BTTB_Hyz}
\end{equation}
And each diagonal of the blocks are represented by $P \times P$ elements $h^{q}_{p}$, $p = -P + 1, \dots, 0, \dots, P - 1$:
\begin{equation}
	\mathbf{H}^{q}_\mathbf{yz} =  \{h^{q}_p\} = \begin{bmatrix}
		h^{q}_{0}   & h^{q}_{-1} & \cdots    & h^{q}_{-P+1} \\
		h^{q}_{1}   & h^{q}_{0} & \ddots    & \vdots           \\ 
		\vdots      & \ddots    & \ddots    & h^{q}_{-1}   \\
		h^{q}_{P-1} & \cdots    & h^{q}_{1} & h^{q}_{0}                 
	\end{bmatrix}_{P \times P} \: .
	\label{eq:Hyz_block}
\end{equation}
In a $x$-\textit{oriented grid} $Q = N_{x}$, $P = N_{y}$ and $q$ and $p$ can be defined by the functions:
\begin{equation}
	q(i, j) = \; \mid l(i) - l(j) \mid
	\label{eq:Hyz-q-x-oriented}
\end{equation}
and
\begin{equation}
	p(i, j) = \; k(i) - k(j) \quad ,
	\label{eq:Hyz-p-x-oriented}
\end{equation}
where $l(i)$ and $l(j)$ are defined by equation \ref{eq:l-x-oriented} 
and $k(i)$ and $k(j)$ are defined by equation \ref{eq:k-x-oriented}.
For $y$-oriented grids, $Q = N_{x}$, $P = N_{y}$ and the block indices
$q$ and $p$ are defined, respectively, by the following integer functions 
of the matrix indices $i$ and $j$:
\begin{equation}
	q(i, j) = \; \mid k(i) - k(j) \mid
	\label{eq:Hyz-q-y-oriented}
\end{equation}
and
\begin{equation}
	p(i, j) = \; l(i) - l(j) \quad ,
	\label{eq:Hyz-p-y-oriented}
\end{equation}
Being a symmetric matrix by blocks, the values of $\mathbf{H_{yz}}$ from oposing diagonals blocks are equal, but each block have skew symmetric oposing diagonals, i.e., $h^{q}_{-1} = - h^{q}_{1}$.

%======================================================================================
\subsection{CGLS matrix-vector substitution}
%======================================================================================

As pointed earlier in this work, the main improvement inside the CGLS method (Algorithm \ref{al:std-cgls-algorithm}) for estimating the parameter vector $\hat{\mathbf{p}}$ (equation \ref{eq:estimated-p-parameter-space}) is to substitute the matrix-vector multiplication $\mathbf{A}^{\top} \mathbf{s}^{(0)}$ out of the loop and the two matrix-vector multiplications inside the loop at steps 3 an 6, $\mathbf{A} \, \mathbf{c}^{(it)}$ and $\mathbf{A}^{\top} \, \mathbf{s}^{(it + 1)}$, that is necessary at each iteration and takes most of its runtime.

Our method consists in calculating the six first columns of the second derivatives of $\mathbf{H}$ (equation \ref{eq:Hi}) and embbed them into the first six columns of the block-circulant circulant-block (BCCB) matrices related to the $\mathbf{H}$ components. Thus, it is possible to calculate the first column of the BCCB matrix embbeded from matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) by multiplying each component with its respective constants and summing as shown in equation \ref{eq:aij_mag_expand}. In \cite{takahashi2020convolutional}, Appendix A, the authors demonstrated in details how to transform a symmetric BTTB matrix into a BCCB matrix $\mathbf{C}$. The process here is the same and that work can be referenced to achieve the same results.

A new auxuliary linear system is constructed to carry the matrix-vector product:
\begin{equation}
\mathbf{w} = \mathbf{C} \mathbf{v} \: ,
\label{eq:w_Cv}
\end{equation}
where
\begin{equation}
\mathbf{w} = \begin{bmatrix}
\mathbf{w}_{0} \\
\vdots \\
\mathbf{w}_{Q - 1} \\
\mathbf{0}_{2N \times 1}
\end{bmatrix}_{4N \times 1} \quad ,
\label{eq:w-vector}
\end{equation}
\begin{equation}
\mathbf{w}_{q} = \begin{bmatrix}
\mathbf{d}_{q}(\mathbf{p}) \\
\mathbf{0}_{P \times 1}
\end{bmatrix}_{2P \times 1}
\label{eq:wq-vector} \quad ,
\end{equation}
\begin{equation}
\mathbf{v} = \begin{bmatrix}
\mathbf{v}_{0} \\
\vdots \\
\mathbf{v}_{Q - 1} \\
\mathbf{0}_{2N \times 1}
\end{bmatrix}_{4N \times 1} \quad ,
\label{eq:v-vector}
\end{equation}
and
\begin{equation}
\mathbf{v}_{q} = \begin{bmatrix}
\mathbf{p}_{q} \\
\mathbf{0}_{P \times 1}
\end{bmatrix}_{2P \times 1}
\label{eq:vq-vector} \quad ,
\end{equation}
where $\mathbf{C}$ (equation \ref{eq:w_Cv}) is a $4N \times 4N$ non-symmetric (BCCB) resulted from transforming $\mathbf{A}$ (equation \ref{eq:aij_mag}). Without having to calculate the whole BCCB matrix, its first column can be used to carry the multiplication of this new system (equation \ref{eq:w_Cv}). Appendix A and C in \cite{takahashi2020convolutional} shows how to use the 2D-FFT to compute the eigenvalues of matrix $\mathbf{C}$, store in a $2Q \times 2P$ matrix using the $vec$-operator and to carry the matrix-vector product. Denoting matrix $\mathbf{L}$ as the eigenvalues matrix follows:
\begin{equation}
	\mathbf{F}_{2Q}^{\ast} \left[ 
	\mathbf{L} \circ \left(\mathbf{F}_{2Q} \, \mathbf{V} \, \mathbf{F}_{2P} \right) 
	\right] \mathbf{F}_{2P}^{\ast} = \mathbf{W} \: ,
	\label{eq:DFT-system}
\end{equation}
where the symbol ``$\circ$'' references the Hadamard product, i.e., a element-wise complex multiplication between the eingenvalues and the 2D-FFT of the matrix rearranged along the rows of the parameters $\mathbf{V}$ (equation \ref{eq:v-vector}) using the same $vec$-operator. The resulting inverse 2D-FFT denoted by $\mathbf{F}_{2Q}^{\ast}  \otimes \mathbf{F}_{2P}^{\ast}$ is also a $2Q \times 2P$ matrix ($\mathbf{W}$) that can be rearranged to the predicted data vector $\mathbf{d}(\hat{\mathbf{p}})$ size $N$.

%======================================================================================
\subsection{Computational performance}
%======================================================================================

To compare the efficiency of our algorithm we will use a numerical approach and calculate the floating-point operations (\emph{flops}), i.e., count the number of mathematical operations necessary to complete the estimative of parameter vector $\mathbf{\hat{p}}$ of the normal equations (equation \ref{eq:estimated-p-parameter-space}) and both the CGLS methods (algorithm \ref{al:std-cgls-algorithm}) for calculating the matrix-vector product by its standart way and our approach.

The \emph{flops} needed to solve the linear system in equation \ref{eq:estimated-p-parameter-space} using the Cholesky factorization is:
\begin{equation}
f_{classical} =  \dfrac{7}{3} N^{3} + 6 N^{2}\: ,
\label{eq:flops-normal-cholesky}
\end{equation}
where $N$ is the total number of observation points and also the size of estimated parameter vector $\mathbf{\hat{p}}$.

For the more efficient CGLS algorithm the estimative can be done in:
\begin{equation}
f_{cgls} =  2 N^{2} + it \, (4 N^{2} + 12 N) \: .
\label{eq:flops-cgls}
\end{equation}
However, our approach reduces further to:
\begin{equation}
f_{ours} =  \kappa  \, 16 N \log_2(4 N) + 24 N + it \, (\kappa  \, 16 N \log_2 (4 N) + 60 N) \: ,
\label{eq:flops-cgls-bccb}
\end{equation}
where $\kappa$ depends on the FFT algorithm. By default, in this work we will use $\kappa = 5$ for the \emph{radix-2} algorithm \citep{vanloan1992}.

Figure \ref{fig:flops} shows a comparative between the methods varying the number of observation points up to $1,000,000$, where it is possible to observe a reduction of $10^7$ orders of magnitude to estimate parameter vector $\mathbf{\hat{p}}$ in relation to the non-iterative classical method and $10^3$ orders of magnitude in relation to the standart CGLS algorithm using $50$ iterations. A more detailed, step by step, flops count of the classical and CGLS algorithm can be found in Appendix A.

In figure \ref{fig:solve_time} we show the time necessary to construct matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}) and solve the linear system up to $10,000$ points of observation. With this dataset the classical method takes more than sixty-three seconds, the CGLS more than twelve seconds, while our method takes only half a second. The cpu used for this test was a intel core i7-7700HQ@2.8GHz.

In figure \ref{fig:sources_time} a comparison between the time to complete the task to calculate the first column of the BCCB matrix embbeded from the from $\mathbf{A}$ (equation \ref{eq:aij_mag}) by using only one equivalent source, i.e., calculating all six first column of the second derivatives matrices from $\mathbf{H}$ (equation \ref{eq:Hi}) and using four equivalent sources to calculate the four necessary columns from the non-symmetric matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}). Although, very similar in time, with one source a small advantage can be observed as the number of data $N$ increases and goes beyond $N = 200,000$. This test was done from $N = 10,000$ to $N = 700,000$ with increases of $5,625$ observation points.

In Table \ref{tab:RAM-usage} there is comparison between how much RAM memory is adressed to store the sensitivity matrix for each of the methods. The classical approach and the CGLS have to store the whole matrix $\mathbf{A}$ (equation \ref{eq:aij_mag}), this means that a dataset  with for example $N = 10,000$ observation points, the sensitivity matrix has $N^2 = 100,000,000$ elements and takes approximately $763$ Megabytes of memory (8 bytes per element). For our method, it is necessary to store the first six columns of each of the components from matrix $\mathbf{H}$ (equation \ref{eq:Hi}) embedded into the BCCB matrices. With the same dataset $N = 10,000$ it needs $1.831$ Megabytes. After completing the steps to store the eigenvalues of matrix $\mathbf{C}$ (equation \ref{eq:w_Cv}) it takes only $0.6104$ Megabytes. Here, we are considering 16 bytes per element as the eigenvalues are complex numbers resulting from the 2D FFT. For a bigger dataset as $N = 1,000,000$ the amount of RAM necessary goes to $7,629,395$, $183.096$ and $61.035$ Megabytes, respectively, showing the necessity to find improved and efficient methods for the equivalent layer technique as the one presented in this work. We remember that throughout our work we are always considering $N = M$.









